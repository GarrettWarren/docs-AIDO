# Residual Learning Policy by Reinforcement Learning {#embodied_rpl status=ready}

This section describes the basic procedure for making a submission with a model trained in simulation using reinforcement learning with PyTorch and the ROS baseline. It can be used as a starting point for any of the [`LF`](#lf), [`LFV`](#lf_v), and [`LFVI`](#lf_v_i) challenges.

## Motivation

One of the most used controller, still to this day, is the PID controller. Despite its simplicity, this controller still manage to perform a vast array of task with reasonnably good performance. However, it still have its limitations, especially in more complexe environment. PID controller are notorious for getting exponentially harder to implement if the task requires more and more subtle adjustments.

On the other hand, one of the most valued benefit of Neural Network(NN) in machine learning is its ability to learn complex function. Altough one the drawback from it is the necessity to have access to a large amount of data in order to well represent complex ditstributions without simply overfitting to the data. This drawback is particularly present in the context of reinforcement learning where the data isn't usually straight up available, but must be generated by exploring the environement using a certain  policy. In the context of reinforcement learning not only does the data requirement to learn a model free base policy is restrictive, but the training itself can be hard and unsuccesful because of the complexity of the task. To circumvent this, some have tried to pretrain the model free policy on another controller, at first, in order to give him a good starting point. However, these problems persist. 

A relatively new concept called residual policy training (RPL) can be used as an alternative to further improved the data efficiency and reduce the probability of divergence by reducing the variation of the predicted action. Instead of either creating a perfect PID controller or completely modelizing the robot with a deep reinforcement learning model, RPL makes an attempt at combining the two. Like previously said, a basic PID controller can offer reasonnably good performances. So, to palliate to the complexity of training a deep reinforcement learning model from scratch, the PID controller is used as an initial policy. The goal of the model then changes from modelizing the whole system to only a residual that is applied on top of the initial policy. This residual can be of two different nature. "If the initial policy is nearly perfect, the residual may be viewed as a corrective term. But if the initial policy is far from ideal, we may interpret the outputs of $\pi$ as merely “hints” to guide exploration." The residual policy is defined as such 

$$
    \pi_\theta(s) = \pi(s) + f_\theta(s)
$$

&pi;<sub>&theta;</sub>(s) = &pi;(s) + f<sub>&theta;</sub>(s)

Where $\pi_\theta$ is the resulting residual policy, $\pi$ the initial policy and $f_\theta$ the corrective term using RL. When defined as such, it can be seen that $\nabla_{\theta} \pi_{\theta}(s)=\nabla_{\theta} f_{\theta}(s)$ , which mean we can use the policy gradient method to learn $\pi_{\theta}(s)$.


Where &pi;<sub>&theta;</sub> is the resulting residual policy, &pi; the initial policy and f<sub>&theta;</sub> the corrective term using RL. When defined as such, it can be seen that &nabla;<sub>&theta;</sub> &pi;<sub>&theta;</sub>(s)=nabla;<sub>&theta;</sub> f<sub>&theta;</sub>(s) , which mean we can use the policy gradient method to learn &pi;<sub>&theta;</sub>(s).


In the context of controlling a duckiebot, another controller that could be used as an initial policy is Pure Pursuit (PP). Originally designed for aerial combat, this can be used in the context of autonomous vehicule by following an imaginary point that is always at a certain distance on the path that needs to be followed. The controller works by constantly tracing a curve that connects the vehicule and the target point. This curve represents the movement the robot should execute.

## Setup

1) Clone [this repo](https://github.com/PhilippeMarcotte/challenge-aido_LF-baseline-RPL-duckietown)

    $ git clone git@github.com:PhilippeMarcotte/challenge-aido_LF-baseline-RPL-duckietown.git
    
2) Change into the develop directory
    
    $ cd challenge-aido_LF-baseline-RPL-sim-pytorch/1_develop

3) Pull the submodules

    $ git submodule init
    $ git submodule update
    $ git submodule foreach "(git checkout daffy; git pull)"
        
4) Setup the docker containers (first time will take a while)

    $ docker-compose up

## How to train a residual policy
        
1) Start a basic policy (example Pure Pursuit)

    $ docker exec -it 1_develop_lanefollow_1 /bin/bash
    $ source /opt/ros/melodic/setup.bash
    $ catkin build --workspace catkin_ws
    $ source catkin_ws/devel/setup.bash
    $ roslaunch custom/lf_slim_pp.launch

2) Start simulation in training mode

    $ docker exec -it 1_develop_lanefollow_1 /bin/bash
    $ ./launch_sim.sh --training

The checkpoints are saved in `catkin_ws/pytorch_models`. The tensorboard logs and numpy array of rewards are in `catkin_ws/results`.

It is possible to modify the hyperparameters of the training in `1_develop/sim_ws/src/gymdt/scripts/duckietown_rl/args.py`.

## How to run a trained policy locally in the simulator

1) Copy the checkpoints.

    $ cp catkin_ws/pytorch_models/* catkin_ws/src/rl_model/models/

1) Start the rl model and basic controller.

    $ docker exec -it 1_develop_lanefollow_1 /bin/bash
    $ source /opt/ros/melodic/setup.bash
    $ catkin build --workspace catkin_ws
    $ source catkin_ws/devel/setup.bash
    $ roslaunch custom/lf_slim_rl.launch

2) Start simulation normally.

    $ docker exec -it 1_develop_lanefollow_1 /bin/bash
    $ ./launch_sim.sh

The `lf_slim_rl` launch file will start all the nodes necessary for the lane following including the basic Pure Pursuit controller and the node applying the residual.

## How to submit the basic controller and the trained policy

Using the Pure Pursuit controller as an example of basic controller and starting from the root of the repo.

1) Copy the latest checkpoints.

    $ cp 1_develop/catkin_ws/pytorch_models/* 1_develop/catkin_ws/src/rl_model/models/

2) Copy the basic controller package and the RL model package to the submit folder.

    $ cp -r 1_develop/catkin/src/pp/packages/lane_pp 3_submit/
    $ cp -r 1_develop/catkin/src/rl_model 3_submit/

3) Copy the launch file.

    $ cp 1_develop/custom/lf_slim_rl.launch 3_submit/

4) Change into the submit directory.

    $ cd 3_submit

5) Submit locally or to the server.

Locally

    $ dts challenges evaluate --challenge aido3-LF-sim-validation

Server

    $ dts challenges submit