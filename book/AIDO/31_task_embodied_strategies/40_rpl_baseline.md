# Residual Learning Policy by Reinforcement Learning {#embodied_rpl status=ready}

This section describes the basic procedure for making a submission with a model trained in simulation using reinforcement learning with PyTorch and the ROS baseline. It can be used as a starting point for any of the [`LF`](#lf), [`LFV`](#lf_v), and [`LFVI`](#lf_v_i) challenges.

## Motivation

Some of the most used controller, still to this day, are basic controllers like PID and Pure Pursuit(PP). Despite their simplicity, these controllers still manage to perform a vast array of tasks with reasonnably good performance. However, they still have their limitations, especially in more complex environment. These controllers are notorious for getting exponentially harder to implement if the task requires more and more subtle adjustments.

On the other hand, one of the most valued benefit of Neural Network(NN) in machine learning is its ability to learn complex function. Altough one the drawback from it is the necessity to have access to a large amount of data in order to well represent complex ditstributions without simply overfitting to the data. This drawback is particularly present in the context of reinforcement learning where the data isn't usually straight up available, but must be generated by exploring the environement using a certain  policy. In the context of reinforcement learning not only does the data requirement to learn a model free base policy is restrictive, but the training itself can be hard and unsuccesful because of the complexity of the task. To circumvent this, some have tried to pretrain the model free policy on another controller, at first, in order to give him a good starting point. However, these problems persist. 

A relatively new concept called residual policy training ([RPL](https://arxiv.org/abs/1812.06298)) can be used as an alternative to further improved the data efficiency and reduce the probability of divergence by reducing the variation of the predicted action. Instead of either creating a perfect basic controller or completely modelizing the robot with a deep reinforcement learning model, RPL makes an attempt at combining the two. Like previously said, a basic controller can offer reasonnably good performances. So, to palliate to the complexity of training a deep reinforcement learning model from scratch, the basic controller is used as an initial policy. The goal of the model then changes from modelizing the whole system to only a residual that is applied on top of the initial policy as seen in the picture below. This residual can be of two different nature. "If the initial policy is nearly perfect, the residual may be viewed as a corrective term. But if the initial policy is far from ideal, we may interpret the outputs of $\pi$ as merely “hints” to guide exploration." The residual policy is defined as such 

$$
    \pi_\theta(s) = \pi(s) + f_\theta(s)
$$

Where $\pi_\theta$ is the resulting residual policy, $\pi$ the initial policy and $f_\theta$ the corrective term using RL. When defined as such, it can be seen that $\nabla_{\theta} \pi_{\theta}(s)=\nabla_{\theta} f_{\theta}(s)$ , which mean we can use the policy gradient method to learn $\pi_{\theta}(s)$.
<p align="center">
<img src="images/RPL_pipeline.png" height="150"/>
</p>
    
 ## Methodology

In the context of controlling a duckiebot, the basic controller use in this baseline is a [PP controller](https://github.com/tobicarvalho/pp/blob/v1/RPL_project_report.pdf) since it has already demonstrated its capabilities.

The RPL task is to predict a correction for each wheel velocity based on the image captured by the camera. The reinforcement learning model is an adaptation of [DDPG](https://arxiv.org/abs/1509.02971). The critic and the actor are 4 layers deep CNNs ending with 3 fully connected layers. The actor outputs two values, one for each wheel, which finally go through a tanh function meaning that the corrections are beetween -1 and 1. During the training, the critic receives this correction and the camera image in order to output the corresponding reward.


## Setup

1) Clone [this repo](https://github.com/PhilippeMarcotte/challenge-aido_LF-baseline-RPL-duckietown)

    $ git clone git@github.com:PhilippeMarcotte/challenge-aido_LF-baseline-RPL-duckietown.git
    
2) Change into the develop directory
    
    $ cd challenge-aido_LF-baseline-RPL-sim-pytorch/1_develop

3) Pull the submodules

    $ git submodule init
    $ git submodule update
    $ git submodule foreach "(git checkout daffy; git pull)"
        
4) Setup the docker containers (first time will take a while)

    $ docker-compose up

## How to train a residual policy
        
1) Start a basic policy (example Pure Pursuit)

    $ docker exec -it 1_develop_lanefollow_1 /bin/bash
    $ source /opt/ros/melodic/setup.bash
    $ catkin build --workspace catkin_ws
    $ source catkin_ws/devel/setup.bash
    $ roslaunch custom/lf_slim_pp.launch

2) Start simulation in training mode

    $ docker exec -it 1_develop_lanefollow_1 /bin/bash
    $ ./launch_sim.sh --training

The checkpoints are saved in `catkin_ws/pytorch_models`. The tensorboard logs and numpy array of rewards are in `catkin_ws/results`.

It is possible to modify the hyperparameters of the training in `1_develop/sim_ws/src/gymdt/scripts/duckietown_rl/args.py`.

## How to run a trained policy locally in the simulator

1) Copy the checkpoints.

    $ cp catkin_ws/pytorch_models/* catkin_ws/src/rl_model/models/

1) Start the rl model and basic controller.

    $ docker exec -it 1_develop_lanefollow_1 /bin/bash
    $ source /opt/ros/melodic/setup.bash
    $ catkin build --workspace catkin_ws
    $ source catkin_ws/devel/setup.bash
    $ roslaunch custom/lf_slim_rl.launch

2) Start simulation normally.

    $ docker exec -it 1_develop_lanefollow_1 /bin/bash
    $ ./launch_sim.sh

The `lf_slim_rl` launch file will start all the nodes necessary for the lane following including the basic Pure Pursuit controller and the node applying the residual.

## How to submit the basic controller and the trained policy

Using the Pure Pursuit controller as an example of basic controller and starting from the root of the repo.

1) Copy the latest checkpoints.

    $ cp 1_develop/catkin_ws/pytorch_models/* 1_develop/catkin_ws/src/rl_model/models/

2) Copy the basic controller package and the RL model package to the submit folder.

    $ cp -r 1_develop/catkin/src/pp/packages/lane_pp 3_submit/
    $ cp -r 1_develop/catkin/src/rl_model 3_submit/

3) Copy the launch file.

    $ cp 1_develop/custom/lf_slim_rl.launch 3_submit/

4) Change into the submit directory.

    $ cd 3_submit

5) Submit locally or to the server.

Locally

    $ dts challenges evaluate --challenge aido3-LF-sim-validation

Server

    $ dts challenges submit